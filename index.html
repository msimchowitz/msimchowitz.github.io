<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Max Simchowitz</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Max Simchowitz</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Max Simchowitz</h1>
</div>
<table class="imgtable"><tr><td>
<img src="prof_new.jpg" alt="Max Simchowitz" width="250px" />&nbsp;</td>
<td align="left"><p>Max Simchowitz <br />
msimchow(AT)andrew(DOT)cmu(DOT)edu<br />
<a href="maxs_cv.pdf">CV</a><br />
<a href="https://scholar.google.com/citations?user=QhG_7egAAAAJ&amp;hl=en">Google Scholar profile</a><br /></p>
</td></tr></table>
<h2>About Me</h2>
<p><b>I am an incoming assistant professor in the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> at Carnegie Mellon University</b>, beginning in January 2025. I will be spending this Fall as a visiting researcher at the Modern Paradigms in Generalization program at the Simons Institute for the Theory of Computing, at UC Berkeley. Moreover:

<b><font color="red">I am  actively recruiting students applying for PhD programs this year to begin in Fall of 2025! </font></b></p>

<p>My current work focuses on the theoretical foundations of learning for robotics and how to operationalize these insights in practical algorithms; this research has motivated a parallel interest in learning and extrapolation under distribution shift. My current work draws on past research ranging broadly across topics in adaptive sampling, multi-arm bandits, complexity of convex and non-convex optimization,  reinforcement learning, learning in linear and nonlinear dynamical systems, and fairness in machine learning.  
</p>
<p>
	I was formerly a
 postdoc in Russ Tedrake's <a href = https://locomotion.csail.mit.edu/people.html>Robot Locomotion Group</a> in the <a href="https://www.eecs.mit.edu/">EECS Department</a>  at MIT. 
I received my PhD in the EECS department at UC Berkeley, co-advised by <a href="http://www.eecs.berkeley.edu/~brecht/">Ben Recht</a> and <a href="http://www.cs.berkeley.edu/~jordan/">Michael Jordan</a>, where I was generously supported by Open Philanthropy, NSF GRFP grant and Berkeley Fellowship grants. I also worked with, and was closely mentored by, <a href="https://ehazan.com/">Elad Hazan</a> at Princeton and <a href="https://homes.cs.washington.edu/~jamieson/about.html">Kevin Jamieson</a>, now at University of Washington. Previously, I recieved a BA in Mathematics at Princeton University, where I was fortunate enough to do research with <a href="https://www.cs.princeton.edu/~arora/">Sanjeev Arora</a> and <a href="http://www.cs.columbia.edu/~blei/">David Blei</a> (who taught at Princeton at the time).  
 </p>
</ul>
<h2>Selected Works</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2407.01392"> Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion
</a><br /> Boyuan Chen, Diego Marti Monso, Yilun Du, <b>Max Simchowitz</b>, Russ Tedrake, Vincent Sitzmann.
<i>Under Submission</i>, 2024.</p>
<li><p><a href="https://arxiv.org/abs/2307.14619
">Provable Guarantees for Generative Behavior Cloning: Bridging Low-Level Stability and High-Level Behavior
 </a><br />Adam Block*, Ali Jadbabaie, Daniel Pfrommer*, <b>Max Simchowitz*</b>, Russ Tedrake<i>. Neurips</i>, 2023. <i>(* denotes equal contribution)</i><br /> 
 <i>Former title, currently used by Google Scholar: "Imitating Complex Behavior: Bridging Low-Level Stability and High-Level Behavior"</i> </p>

 <li><p><a href="http://arxiv.org/abs/2302.13934">Statistical Learning under Heterogeneous Distribution Shift
 </a><br /><b>Max Simchowitz</b>, Anurag Ajay, Pulkit Agrawal, Akshay Krishnamurthy. <i>ICML</i>, 2023. </p>
 <li><p><a href="https://arxiv.org/pdf/2202.00817.pdf"> Do Differentiable Simulators Give Better Policy Gradients?
</a><br /> H.J. Terry Suh, <b>Max Simchowitz</b>, Kaiqing Zhang, Russ Tedrake. <i>ICML</i>, <font color = "red"><b>Outstanding Paper Award</b></font>, 2022. </p>
<li><p><a href="https://arxiv.org/abs/2001.09576">Naive Exploration is Optimal for Online LQR</a><br /> <b>Max Simchowitz</b>, Dylan Foster. <i>ICML</i>, 2020. </p>
<li><p><a href="https://arxiv.org/abs/2001.09254">Improper Learning for Nonstochastic Control</a><br />  <b>Max Simchowitz</b>, Karan Singh, Elad Hazan. <i>COLT</i>, 2020. </p>
<li><p><a href="https://arxiv.org/abs/1905.03814">Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs
</a><br />  <b>Max Simchowitz</b>, Kevin Jamieson. <i>NeurIPS</i>, 2019. </p>
<li><p><a href="https://arxiv.org/pdf/1802.08334">Learning Without Mixing: Towards A Sharp Analysis of Linear System Identification</a> <br />
<b>Max Simchowitz</b>, Horia Mania, Stephen Tu, Benjamin Recht, Michael I. Jordan. <i>COLT</i>, 2018.<br /></p>
</ul>

<h2>Teaching</h2>
<p><b>CS 189/289A</b>, Introduction to Machine Learning, UC Berkeley Fall 2018 (TA). </p>
<p><b>EE227C</b>, <a href="https://ee227c.github.io/">Convex Optimization and Approximation</a>, UC Berkeley, Spring 2018 (TA). <a href="https://ee227c.github.io/notes/ee227c-notes.pdf">Link for course notes</a>. </p>
<div id="footer">
<div id="footer-text">
Page generated 2018-08-08 15:54:44 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
